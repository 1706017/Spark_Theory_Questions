Spark Theory Questions:
========================
1.What is spark ?
2.What is RDD?
3.What is DAG and How it is useful in spark code ?
4.What are the two types of operations performed in spark?
5.what happens as per spark cluster when we call sc.textFile("file1.csv") for the first time do some visualization wrt spark 4 node cluster?
6.How rdds are resilient to failures ?
7.Why rdds are immutable and How immutablity is beneficial for rdds ?
8.why transformations are lazy ?
9.What is Spark Context ?
10.what is the port number where we can see the spark UI after execution ?
 -> localhost:4040/

11.which action is used in production environment rdd.collect() or rdd.saveAsTextFile(" hdfs folder path where processed data will be stored") and why ?
12.Which is better map+reduceByKey or countByValue?

Sol: map+reduceByKey is a transaformation and a transaformation always return an rdd 
     on the other hand countByValue is an action and an action always returns a local variable 
     so we should use action only when the step that we are  going to perform is last step other wise if we still need to perform more action after that step then we should always go for transformation that is map + reduceByKey

13.What is YARN and why it is required ?

14.How to execute the spark programs on spark cluster?
Sol: we can execute the spark program on spark cluster in two ways
    a) Interactive mode : by using spark-shell for scala / pyspark for spark in python
    b) By submitting a job : by using spark-submit 

15.How does spark execute our program on the cluster ?

Sol: Spark follows master slave architecture where each application have one master node/driver node and have bunch of worker node / executors

the main job of driver node is to distribute the task , monitors the executors ,analyse the work etc
and executors are the machines which are actually executing the spark code 
