Spark Theory Questions:
========================
1.What is spark ?
2.What is RDD?
3.What is DAG and How it is useful in spark code ?
4.What are the two types of operations performed in spark?
5.what happens as per spark cluster when we call sc.textFile("file1.csv") for the first time do some visualization wrt spark 4 node cluster?
6.How rdds are resilient to failures ?
7.Why rdds are immutable and How immutablity is beneficial for rdds ?
8.why transformations are lazy ?
9.What is Spark Context ?
10.what is the port number where we can see the spark UI after execution ?
 -> localhost:4040/

11.which action is used in production environment rdd.collect() or rdd.saveAsTextFile(" hdfs folder path where processed data will be stored") and why ?
12.Which is better map+reduceByKey or countByValue?

Sol: map+reduceByKey is a transaformation and a transaformation always return an rdd 
     on the other hand countByValue is an action and an action always returns a local variable 
     so we should use action only when the step that we are  going to perform is last step other wise if we still need to perform more action after that step then we should always go for transformation that is map + reduceByKey

13.What is YARN and why it is required ?

14.How to execute the spark programs on spark cluster?
Sol: we can execute the spark program on spark cluster in two ways
    a) Interactive mode : by using spark-shell for scala / pyspark for spark in python
    b) By submitting a job : by using spark-submit 

15.How does spark execute our program on the cluster ?

Sol: Spark follows master slave architecture where each application have one master node/driver node and have bunch of worker node / executors

the main job of driver node is to distribute the task , monitors the executors ,analyse the work etc
and executors are the machines which are actually executing the spark code 

                                         DATE: 29TH MARCH 2024
===============================================================================================================

16.What is Big Data ?
sol: Any data that is characterize by 4 v's Like volume,variety,velocity and veracity is called Big data 

17.Why do we need Big Data?
sol: We need Big Data to store such huge volume of data and to process such huge volume of data which our traditional systems are not capable to handle 

18.What is the Big Data System Requirements ?
Sol: 1) Store : Should be able to store such huge volume of data
     2) Process: Should be capable to process such Huge amount of data 
     3) Scalable : whenever required could be able to scale up 

19.What are the two ways to build a system ?
Sol: There are two ways to build a system 
    a) Monolithic 
    b) Distributed 

    Monolithic:
    -----------
    In Monolithic system it is simply a Large single system with large number of resources 
    But it has a limitation that after some time it could not be scaled up that is we cannot add more number of resources to this system

    It follows Vertical scaling  
    

    Distributed:
    -------------
    In Distributed system there are many systems with lot of resources but the advantage of distributed over monolithic is that we can add more and more number of resources so it is highly scalable when compared with Monolithic way of desiging system

    It follows Horizontal Scaling 

20.What is HADOOP ?
sol: Haddop is a framework that is used to solve big data problems related to storage,process

21.What is the Diff between Hadoop 1.0 vs Hadoop 2.0 ?
Sol: The main point of difference between Hadoop 1.0 and Hadoop 2.0 

     Hadoop 1.0 
     -----------
     It has two core components :
     a) HDFS (Hadoop Distributed File system) used for distributed storage 
     b) Map Reduce(Used for Distributed processing)

     It has a default block size of 64mb
     
     Hadoop 2.0
     -----------
     It has three core components:
     a)HDFS
     b)Map Reduce
     c)YARN (YET ANOTHER RESOURCE NEGOTIATER) : resource Manager

     It has a default block size of 128mb

22.What is Haddop Ecosystem ?
Sol: It consists of multiple technologies like:
    1) SQOOP -> DATA INGESTION tools helpful for transferring data from RDBMS TO Hadoop (HDFS) vice versa
    2) Hive -> Data warehouse tool where we can query and get our info by writing sql like query called HQL known as HIVE QUERY LANGUAGE
    3) OOZIE -> It is a wokflow schdular that helps to schdule the spark jobs and execute them on the schduled time 
    4) Apache Spark
    5)HBASE: NOsql database

23. Describe HDFS Architecture?
Sol: It consists of One master node and multiple data nodes

Suppose we have a file of 500mb so it will get distribued as 128mb into 4 blocks and each block will be by default 128mb size and they will be stored into 4 data nodes 

and about these blocks all the info is stored in the master node which knows which block is in which data node

Also to overcome the failure case of data nodes each block has been replicated for 3 times and each replicated block is stored on differnt data nodes due to which it is resistance to loss of failure 

24.What is the HDFS Command to copy the file from local to hadfs and copy from hdfs to local ?
sol: 
  A) To copy the file from local to hdfs :
     $hadoop fs -copyFromLocal /user/cloudera/file1.txt /data/

  B) To copy the data file from Hdfs to local :

     $hadoop fs -copyToLocal /data/file1.txt /user/cloudera/input/ 

Q25) What is narrow and wide transformation ?

Sol: In Narrow transformation no shuffling of data is involved
     example: map(),filter(),flatMap()

     In Wide Transformation shuffling of data is involved 
     example: reduceByKey()

Q26) What is shuffling means?
Sol: Shuffling means transfer of data from one machine to other machine

Q27) How does spark stages gets created?
Sol: Whenever we are calling a wide transformation like reduceByKey a new stage gets created in DAG 

Q28) If we use 3 wide Transformations how many stages gets created in Spark DAG?
Sol: 4 stages gets created because previous one will be there already and 3 more gets created as we are using three wide transformations

Q29)What is the differnce between reduceByKey() and groupByKey()?
Sol: Both reduceByKey() and groupByKey() they are wide transformation but the only difference is that in reduceByKey() local aggregation happens due to which it shuffled less data whereas In groupByKey() no local aggregation heppens and due to which it shuffled more data and hence more timetaking and costly operation as shuffling is involve

Q30) What is cache() and persist() ?

Sol: cache() and persist() both are used to cache the rdd in memory so that again from the starting it does not require to calculate the rdd transformation 

Ultimately which helps in speeding up the appllication 

Both cache() and persist() is equivalent only only difference is that cache() cached the rdd in memory whereas persist() comes with various storage levels

Q31) What are different storage levels of persist() ?
Sol: Different storage levels are as follows:
     a) MEMORY_ONLY : In this data is cached in memory only in non serialized format that is in object format which takes more storage

     b) DISK_ONLY : In this data is cached on disk only in serialized format that is in bytes format and it takes less storage

     c) MEMORY_AND_DISK : In this data is cached in memory and if enough memory is not available then it removes from memory and put to disk 
     
     Example for syntax:
     persist(StorageLevel.MEMORY_ONLY)
Q32) Discuss about the apache spark ecosystem ?

Sol: It consists of below components :
     a) spark core : that is basically rdd 
     b) spark SQL : it contains dataframes and dataset api 
     c) Spark streaming: It basically helps to deal with continiously coming data / real time data 
     d) MLib : to support machine learning 
     e) Graphx : for plotting graph and visualization 

Q33) What all transformations and actions you have used till now ?

Sol: Transformations used are below :
     a) map()
     b) reduceByKey()
     c) flatMap()
     d) filter()
     e) groupByKey()
     f) mapValues()
     g) sortByKey()

     Actions used are below:
     a)collect()
     b)saveAsTextFile()

Q34) What is a dataframe in spark ?
Sol: 
     Dataframe is conceptually equivalent to table in rdbms but the only difference between table and dataframe is the table is only inside one machine whereas the dataframe is distributed on multiple machine 
     
     ->Internally dataframe is using rdd only 

Q35) What is the difference between rdd and df ?
Sol: Main difference is that in rdd no schema is attached whereas in df schema is attached 
     
     -> RDD is called as lower level api 
     -> Dataframes are called as Higher level structed api

Q36) What is spark Session ?
Sol: 
    SparkSession is a unified entry point for a spark application and it is benefical as  insead of having multiple context like sql context,hive context and spark context it is better to have a spark session as it encapuslates all the context 
 
    SparkSession is singleton so for one application only one spark session will be there 

Q37) Write a spark code to read the data from csv file and to infer the schema and treat first row as header from the data and write the code using spark dataframe ?

sol:
     object Dataframeexample extends App
     {
       val sparkConf = new SparkConf()

       sparkConf.set("spark.app.name","myapp1")
       sparkConf.set("spark.master","local[2]") //here 2 means two 2 cpu cores 


       val spark = SparkSession.builder()
                   .config(sparkConf)
                   .getOrCreate()

       val OrdersDf = spark.read
                           .option("inferSchema",true)
            			   .option("header",true)
            			   .csv("/user/cloudera/orders.csv")

        OrdersDf.printSchema()

        OrdersDf.repartition(4)
                .select("order_id","orders_customer_id","status")
                .where("status = 'Completed'")
                
        OrdersDf.show()

     }
